{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kipsangmarion/webscraper/blob/main/WebScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PolC566NB36G"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QiLx8S1L_2BA"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import concurrent.futures\n",
    "from re import sub\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdab7Nq3_kzD"
   },
   "source": [
    "# Extracting the links to individual project details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R0M7vk_0ZsUg"
   },
   "outputs": [],
   "source": [
    "# Replace with the URL of the website you want to scrape\n",
    "base_url = 'https://www.thegef.org/projects-operations/database?f%5B0%5D=focal_areas%3A2207&search=&page={}'\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 77\n",
    "\n",
    "# Initialize project_links list\n",
    "project_links = []\n",
    "\n",
    "# Loop through pages\n",
    "for page in range(num_pages):\n",
    "  url = base_url.format(page)\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Find the table that contains the links (you may need to inspect the HTML)\n",
    "  table = soup.find('table')\n",
    "\n",
    "  # Loop through rows in the table\n",
    "  for row in table.find_all('tr'):\n",
    "    # Check if there are any <td> elements in this row\n",
    "    td_elements = row.find_all('td')\n",
    "    if td_elements:\n",
    "      # Assuming the links are in the first column (index 0)\n",
    "      link = td_elements[0].find('a')\n",
    "      if link:\n",
    "        link_text = link.get('href')\n",
    "        full_link = \"https://www.thegef.org\" + link_text\n",
    "        project_links.append(full_link)\n",
    "\n",
    "# Create a DataFrame from project_links and save it to a CSV file\n",
    "link_dataframe = pd.DataFrame(project_links)\n",
    "link_dataframe.to_csv('project_links.csv', index=False, header=None)\n",
    "\n",
    "# print(project_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpXW0R3OCdTq"
   },
   "source": [
    "# Extract project general info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3irsm_nqYtn"
   },
   "source": [
    "Including retries and catching any exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqy7_0VH1Fn5"
   },
   "outputs": [],
   "source": [
    "# Function to scrape project information from a single link\n",
    "def scrape_project_info(project_link):\n",
    "    project_info = {}\n",
    "    # print(project_link)\n",
    "    try:\n",
    "\n",
    "        response = requests.get(project_link)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Project title\n",
    "        title_element = soup.find('span', class_='field--name-title')\n",
    "        title_text = title_element.get_text(strip=True) if title_element else None\n",
    "        project_info['Title'] = title_text\n",
    "\n",
    "        # Funding institution\n",
    "        funding_institution_element = soup.find('div', class_='field--name-field-trust-fund-name')\n",
    "        funding_institution_items = funding_institution_element.find_all('div', class_='field__item')\n",
    "        funding_institution_list = [item.get_text(strip=True) for item in funding_institution_items]\n",
    "        project_info['Funding Institution'] = funding_institution_list\n",
    "\n",
    "        # Implementing institution\n",
    "        implementing_institution_element = soup.find('div', class_='field--name-field-implementing-agencies')\n",
    "        implementing_institution_items = implementing_institution_element.find_all('div', class_='field__item')\n",
    "        implementing_institution_list = [item.get_text(strip=True) for item in implementing_institution_items]\n",
    "        project_info['Implementing Institution'] = implementing_institution_list\n",
    "\n",
    "        # Country\n",
    "        country_element = soup.find('div', class_='field--name-field-country')\n",
    "        country_items = country_element.find_all('div', class_='field__item')\n",
    "        country_list = [item.get_text(strip=True) for item in country_items]\n",
    "        project_info['Country'] = country_list\n",
    "\n",
    "        # Region\n",
    "        region_element = soup.find('div', class_='field--name-field-region')\n",
    "        region_items = region_element.find_all('div', class_='field__item')\n",
    "        region_list = [item.get_text(strip=True) for item in region_items]\n",
    "        project_info['Region'] = region_list\n",
    "\n",
    "        # Status\n",
    "        status_element = soup.find('div', class_='field--name-field-latest-timeline-status')\n",
    "        status_text = status_element.find('div', class_='field__item').get_text(strip=True) if status_element else None\n",
    "        project_info['Status'] = status_text\n",
    "\n",
    "        # Total amount\n",
    "        gef_project_grant_element = soup.find('div', class_='views-field-field-gef-project-grant')\n",
    "        cofinancing_element = soup.find('div', class_='views-field-field-co-financing-total')\n",
    "        gef_project_grant_amount = gef_project_grant_element.find('div', class_='field-content').get_text(strip=True)\n",
    "        cofinancing_amount = cofinancing_element.find('div', class_='field-content').get_text(strip=True)\n",
    "        total_amount = Decimal(sub(r'[^\\d.]', '', gef_project_grant_amount)) + Decimal(sub(r'[^\\d.]', '', cofinancing_amount))\n",
    "        project_info['Total Project Amount'] = str(total_amount)\n",
    "\n",
    "        # Link to project documentation\n",
    "        project_documents_element = soup.find('div', class_='field--name-field-document-url')\n",
    "        project_documents_items = project_documents_element.find_all('div', class_='field__item')\n",
    "        document_links = [item.find('a')['href'] for item in project_documents_items]\n",
    "        project_info['Link to Project Documentation'] = document_links\n",
    "\n",
    "        # Start Date\n",
    "        start_date_element = soup.find('div', class_='views-field-field-combined-project-appr-date')\n",
    "        start_date_text = start_date_element.find('div', class_='field-content').get_text(strip=True) if start_date_element else None\n",
    "        project_info['Start Date'] = start_date_text\n",
    "\n",
    "        # End Date\n",
    "        end_date_element = soup.find('div', class_='views-field-field-combined-closing-date')\n",
    "        end_date_text = end_date_element.find('div', class_='field-content').get_text(strip=True) if end_date_element else None\n",
    "        project_info['End Date'] = end_date_text\n",
    "\n",
    "        return project_info\n",
    "    except Exception as e:\n",
    "        # print(f\"Error scraping project: {e}\")\n",
    "        return None\n",
    "\n",
    "# Number of threads to use for parallel scraping\n",
    "num_threads = 8\n",
    "\n",
    "# Initialize project_info list\n",
    "project_info_list = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    # Submit scraping tasks for each project link\n",
    "    future_to_link = {executor.submit(scrape_project_info, project_link): project_link for project_link in project_links}\n",
    "    for future in concurrent.futures.as_completed(future_to_link):\n",
    "        project_info = future.result()\n",
    "        if project_info:\n",
    "            project_info_list.append(project_info)\n",
    "            # print(project_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgBoFzNWZhuj"
   },
   "source": [
    "# Create dataframe and export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUn3q9IeqRfj"
   },
   "source": [
    "For the purposes of downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2ciwSg5bS1UD"
   },
   "outputs": [],
   "source": [
    "with open(\"thegef_data.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"Title\", \"Funding Institution\", \"Implementing Institution\", \"Country\", \"Region\", \"Status\", \"Total Project Amount\", \"Link to Project Documentation\", \"Start Date\", \"End Date\"]\n",
    "    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    for project_data in project_info_list:\n",
    "        csv_writer.writerow(project_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNPcdLuSiMhsg2X7/DnfBKf",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
